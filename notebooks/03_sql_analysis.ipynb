{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91071bc3",
   "metadata": {},
   "source": [
    "### **2.2 SQL Analysis**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Objectives**\n",
    "\n",
    "Demonstrate advanced SQL skills by creating a database-driven analysis workflow and answering complex business questions through structured queries.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7df247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "from psycopg2.extras import execute_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5443444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSqlIntegration(ABC):\n",
    "      \"\"\"\n",
    "      Abstract base class for bank marketing data SQL integration operations.\n",
    "\n",
    "      This class defines the interface for loading bank marketing data into a SQL database\n",
    "      and querying results. Concrete implementations must provide the actual database\n",
    "      interaction logic.\n",
    "\n",
    "      Attributes:\n",
    "            dataPath (str): Path to the data file to be processed\n",
    "      \"\"\"\n",
    "      def __init__(self, dataPath: str) -> None:\n",
    "            \"\"\"\n",
    "            Initialize the data integration handler.\n",
    "            \n",
    "            Args:\n",
    "            dataPath: Path to the bank marketing data file\n",
    "            \"\"\"\n",
    "            self.dataPath = dataPath\n",
    "\n",
    "      @abstractmethod\n",
    "      def createTable(self, connection: psycopg2.connect) -> None:\n",
    "            \"\"\"\n",
    "            Create the bank_marketing table in the database.\n",
    "            \n",
    "            Args:\n",
    "            connection: Active PostgreSQL database connection\n",
    "            \n",
    "            Raises:\n",
    "            psycopg2.Error: If table creation fails\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "      @abstractmethod\n",
    "      def answerQueryAnswer(self, connection:psycopg2.connect, queryString: str, index: List[str]) -> pd.DataFrame:\n",
    "            \"\"\"\n",
    "            Convert SQL query results into a pandas DataFrame.\n",
    "            \n",
    "            Args:\n",
    "            queryRows: List of tuples from SQL query results\n",
    "            index: List of index values for the DataFrame\n",
    "            \n",
    "            Returns:\n",
    "            pd.DataFrame: Formatted DataFrame from query results\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "      @abstractmethod\n",
    "      def insertRecordsIntoTable(self, connection: psycopg2.connect, alreadyAdded: bool, batch_size: int = 100_000) -> None:\n",
    "            \"\"\"\n",
    "            Insert records from the data file into the database table.\n",
    "\n",
    "            Args:\n",
    "            connection: Active PostgreSQL database connection\n",
    "\n",
    "            Raises:\n",
    "            psycopg2.Error: If data insertion fails\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "\n",
    "class EcommerceDataSqlIntegration(DataSqlIntegration):\n",
    "      \"\"\"\n",
    "      Concrete implementation of bank marketing data SQL integration.\n",
    "      \n",
    "      Provides PostgreSQL-specific implementation for loading bank marketing campaign data\n",
    "      and converting query results to pandas DataFrames.\n",
    "      \"\"\"\n",
    "      \n",
    "      def createTable(self, connection: psycopg2.connect) -> None:\n",
    "            \"\"\"\n",
    "            Create the ecommerce_events table with proper schema if it doesn't exist.\n",
    "            \"\"\"\n",
    "            cursor = connection.cursor()\n",
    "            try:\n",
    "                  cursor.execute(\"\"\"\n",
    "                        CREATE TABLE IF NOT EXISTS ecommerce_events (\n",
    "                        event_id BIGSERIAL PRIMARY KEY,\n",
    "                        event_time TIMESTAMPTZ NOT NULL,\n",
    "                        event_type VARCHAR(20) NOT NULL CHECK (event_type IN ('view', 'cart', 'purchase')),\n",
    "                        product_id BIGINT,\n",
    "                        category_id BIGINT,\n",
    "                        brand TEXT,\n",
    "                        price NUMERIC(12,2),\n",
    "                        user_id BIGINT,\n",
    "                        user_session TEXT,\n",
    "\n",
    "                        isFreeItem BOOLEAN,\n",
    "                        isLuxuryItem BOOLEAN,\n",
    "                        isExtremeOutlier BOOLEAN,\n",
    "                        isAbnormal BOOLEAN,\n",
    "                        has_purchase BOOLEAN,\n",
    "                        isMultiDaySession BOOLEAN,\n",
    "                        hasCategoryCode BOOLEAN,\n",
    "\n",
    "                        category_l1 TEXT,\n",
    "                        category_l2 TEXT,\n",
    "                        category_l3 TEXT,\n",
    "                        isLowFrequencyCategory BOOLEAN,\n",
    "\n",
    "                        sessionDuration REAL,\n",
    "                        sessionStartHour SMALLINT,\n",
    "                        sessionDayOfWeek TEXT,\n",
    "                        isWeekendSession BOOLEAN,\n",
    "                        isMidnightActivity BOOLEAN,\n",
    "                        isDirectPurchase BOOLEAN,\n",
    "                        isDirectPurchase_right BOOLEAN,\n",
    "                        isAbandonedCart BOOLEAN,\n",
    "                        advancedToCart BOOLEAN,\n",
    "                        advancedToPurchase BOOLEAN,\n",
    "                        EngagementDepth INTEGER,\n",
    "\n",
    "                        popularityScore REAL,\n",
    "                        categoryConversionRate REAL,\n",
    "                        isInMultiCategories BOOLEAN\n",
    "                        );\n",
    "                  \"\"\")\n",
    "                  connection.commit()\n",
    "            except psycopg2.Error as e:\n",
    "                  connection.rollback()\n",
    "                  raise e\n",
    "\n",
    "      def addTableIndexes(self, connection: psycopg2.connect) -> None:\n",
    "            try:\n",
    "                  cursor = connection.cursor()\n",
    "                  cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_user_id ON ecommerce_events(user_id);\")\n",
    "                  cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_product_id ON ecommerce_events(product_id);\")\n",
    "                  cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_event_type ON ecommerce_events(event_type);\")\n",
    "                  cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_event_time_brin ON ecommerce_events USING BRIN(event_time);\")\n",
    "                  cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_user_event_time ON ecommerce_events(user_id, event_time);\")\n",
    "                  cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_product_event_type ON ecommerce_events(product_id, event_type);\")\n",
    "                  cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_purchase_only ON ecommerce_events(product_id, event_time) WHERE event_type = 'purchase';\")\n",
    "                  cursor.execute(\"ANALYZE ecommerce_events;\")\n",
    "                  connection.commit()\n",
    "\n",
    "            except psycopg2.Error as e:\n",
    "                  connection.rollback()\n",
    "            \n",
    "            finally:\n",
    "                  cursor.close()\n",
    "\n",
    "      def insertRecordsIntoTable(self, connection: psycopg2.connect, alreadyAdded: bool, batchSize: int = 100_000):\n",
    "            if alreadyAdded:\n",
    "                  return\n",
    "\n",
    "            try:\n",
    "                  cursor = connection.cursor()\n",
    "\n",
    "                  # Lazy read Parquet\n",
    "                  lazy_df = pl.scan_parquet(self.dataPath)\n",
    "                  df = lazy_df.collect()\n",
    "                  total_rows = df.height\n",
    "\n",
    "                  # Iterate in chunks with progress bar\n",
    "                  for start in tqdm(range(0, total_rows, batchSize), desc=\"Inserting rows\"):\n",
    "                        end = min(start + batchSize, total_rows)\n",
    "                        chunk = df[start:end]\n",
    "                        data_tuples = [tuple(row) for row in chunk.to_numpy()]\n",
    "\n",
    "                        if data_tuples:\n",
    "                              execute_values(cursor, \"\"\"\n",
    "                                    INSERT INTO ecommerce_events (\n",
    "                                          event_time, event_type, product_id, category_id, brand, price, user_id, user_session,\n",
    "                                          isFreeItem, isLuxuryItem, isExtremeOutlier, isAbnormal, has_purchase, isMultiDaySession,\n",
    "                                          hasCategoryCode,\n",
    "                                          category_l1, category_l2, category_l3,\n",
    "                                          isLowFrequencyCategory,\n",
    "                                          sessionDuration, sessionStartHour, sessionDayOfWeek, isWeekendSession,\n",
    "                                          isMidnightActivity, isDirectPurchase, isDirectPurchase_right,\n",
    "                                          isAbandonedCart, advancedToCart, advancedToPurchase, EngagementDepth,\n",
    "                                          popularityScore, categoryConversionRate,\n",
    "                                          isInMultiCategories\n",
    "                                    ) VALUES %s\n",
    "                              \"\"\", data_tuples)\n",
    "                              connection.commit()\n",
    "\n",
    "            except Exception as e:\n",
    "                  connection.rollback()\n",
    "                  raise e\n",
    "\n",
    "      def addSummaryTables(self, connection: psycopg2.connect) -> None:\n",
    "            try:\n",
    "                  cursor = connection.cursor()\n",
    "                  cursor.execute(\"\"\"\n",
    "                        CREATE MATERIALIZED VIEW IF NOT EXISTS daily_revenue_summary AS\n",
    "                        SELECT\n",
    "                        DATE(event_time) AS event_date,\n",
    "                        SUM(price) AS total_revenue,\n",
    "                        COUNT(*) FILTER (WHERE event_type = 'purchase') AS total_purchases\n",
    "                        FROM ecommerce_events\n",
    "                        WHERE event_type = 'purchase'\n",
    "                        GROUP BY DATE(event_time)\n",
    "                        ORDER BY event_date;\n",
    "                  \"\"\")\n",
    "                  cursor.execute(\"\"\"\n",
    "                        CREATE MATERIALIZED VIEW IF NOT EXISTS product_perf_summary AS\n",
    "                        SELECT\n",
    "                        product_id,\n",
    "                        COUNT(*) FILTER (WHERE event_type = 'view') AS total_views,\n",
    "                        COUNT(*) FILTER (WHERE event_type = 'cart') AS total_cart,\n",
    "                        COUNT(*) FILTER (WHERE event_type = 'purchase') AS total_purchases,\n",
    "                        (COUNT(*) FILTER (WHERE event_type = 'purchase')::decimal \n",
    "                        / NULLIF(COUNT(*) FILTER (WHERE event_type = 'view'), 0)) AS conversion_rate\n",
    "                        FROM ecommerce_events\n",
    "                        GROUP BY product_id\n",
    "                        ORDER BY conversion_rate DESC;\n",
    "                  \"\"\")\n",
    "                  cursor.execute(\"\"\"\n",
    "                        CREATE MATERIALIZED VIEW IF NOT EXISTS customer_rfm AS\n",
    "                        SELECT\n",
    "                        user_id,\n",
    "                        DATE_PART('day', CURRENT_DATE - MAX(event_time) FILTER (WHERE event_type = 'purchase')) AS recency,\n",
    "                        COUNT(*) FILTER (WHERE event_type = 'purchase') AS frequency,\n",
    "                        COALESCE(SUM(price) FILTER (WHERE event_type = 'purchase'), 0) AS monetary\n",
    "                        FROM\n",
    "                        ecommerce_events\n",
    "                        GROUP BY\n",
    "                        user_id\n",
    "                        ORDER BY\n",
    "                        monetary DESC;\n",
    "                  \"\"\")\n",
    "                  connection.commit()\n",
    "\n",
    "            except psycopg2.Error as e:\n",
    "                  connection.rollback()\n",
    "            \n",
    "            finally:\n",
    "                  cursor.close() \n",
    "\n",
    "      def answerQueryAnswer(self, connection: psycopg2.connect, queryString: str, index: List[str]) -> pd.DataFrame:\n",
    "            cursor = connection.cursor()\n",
    "            try:\n",
    "                  cursor.execute(queryString)\n",
    "                  queryRows = cursor.fetchall()\n",
    "                  \n",
    "                  # Debug: Print raw query results\n",
    "                  print(f\"Query returned {len(queryRows)} rows\")\n",
    "                  if queryRows:\n",
    "                        print(\"First row sample:\", queryRows[0])\n",
    "                  \n",
    "                  # Create DataFrame with proper column handling\n",
    "                  if not queryRows:\n",
    "                        return pd.DataFrame(index=index)\n",
    "                        \n",
    "                  # Convert to DataFrame with column names\n",
    "                  df = pd.DataFrame.from_records(\n",
    "                        queryRows,\n",
    "                        columns=[desc[0] for desc in cursor.description],\n",
    "                        index=index[:len(queryRows)]  # Ensure index matches row count\n",
    "                  )\n",
    "                  return df\n",
    "                  \n",
    "            except psycopg2.Error as e:\n",
    "                  print(f\"Database error: {e}\")\n",
    "                  return pd.DataFrame()\n",
    "            finally:\n",
    "                  cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf196337",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSqlIntegration = EcommerceDataSqlIntegration(dataPath=\"D:\\programming\\Data Analysis\\E-commerceCustomerBehaviorAnalysis\\Data\\Processed\\prsc_fe9_nov_2019.parquet\")\n",
    "connection = psycopg2.connect(database=\"ecommerceCustomers\",host=\"localhost\",user=\"postgres\",\n",
    "                                    password=\"postgres\",port=\"5432\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c38386",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSqlIntegration.createTable(connection=connection)\n",
    "dataSqlIntegration.insertRecordsIntoTable(connection=connection, alreadyAdded=False, batchSize=100000)\n",
    "dataSqlIntegration.addTableIndexes(connection=connection)\n",
    "dataSqlIntegration.addSummaryTables(connection=connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61a8021",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **Task 2.2.1 – Database Design and Data Loading**\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Database Schema Design\n",
    "\n",
    "**Q1. What is the optimal table structure to normalize the e-commerce data while maintaining query performance?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* Use a **single wide `ecommerce_events` table** (as you already did) because:\n",
    "\n",
    "  * Data is clean → no need for staging tables.\n",
    "  * Analysis and BI dashboards benefit from one denormalized table.\n",
    "* Keep dimensions (`products`, `customers`, `categories`) optional for future expansion, but not required now.\n",
    "* Code strategy: rely on your `createTable()` implementation that defines the schema with correct data types and constraints.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. How should we design indexes on user\\_id, product\\_id, and event\\_time to optimize common analytical queries?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* Create **BTREE indexes** on:\n",
    "\n",
    "  * `user_id` → for customer-level queries.\n",
    "  * `product_id` → for product performance analysis.\n",
    "  * `event_type` → for funnel/conversion queries.\n",
    "* Create **BRIN index** on `event_time` → for fast filtering by time ranges (cheap and scalable for 285M rows).\n",
    "* Optionally, add composite `(user_id, event_time)` for recency analysis.\n",
    "* Code strategy: extend `createTable()` to run `CREATE INDEX` after table creation.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. What foreign key relationships should be established between customers, products, and events tables?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* Since all analysis happens in the **single wide table**, foreign keys aren’t required for integrity (data is already clean).\n",
    "* If needed in the future:\n",
    "\n",
    "  * `events.user_id → customers.user_id`\n",
    "  * `events.product_id → products.product_id`\n",
    "* Code strategy: skip FK constraints for now to keep inserts fast; revisit if you add dimension tables later.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Data Import and Validation\n",
    "\n",
    "**Q4. How can we efficiently load 285 million records into a relational database with proper data type conversions?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* You already stream from parquet → Polars → numpy tuples → Postgres.\n",
    "* Optimize by:\n",
    "\n",
    "  * Using `execute_values()` in batches (you already do this).\n",
    "  * Tuning `batchSize` (100k is good; test 250k–500k for speed).\n",
    "  * Running with autocommit off + commit per batch (as you implemented).\n",
    "* Code strategy: keep your current `insertRecordsIntoTable()` logic.\n",
    "\n",
    "---\n",
    "\n",
    "**Q5. What SQL constraints should be implemented to ensure data quality during the import process?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* Since data is **already clean**, keep constraints minimal for performance:\n",
    "\n",
    "  * `NOT NULL` only where logically required (`event_time`, `event_type`).\n",
    "  * `CHECK` on `event_type IN ('view','cart','purchase')`.\n",
    "* Skip extra checks like duplicates or null validation (handled upstream in data pipeline).\n",
    "* Code strategy: your current `createTable()` is sufficient.\n",
    "\n",
    "---\n",
    "\n",
    "**Q6. How do we handle duplicate records and maintain referential integrity during bulk data loading?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* No duplicates exist → skip deduplication logic.\n",
    "* Referential integrity is not enforced (since no FK constraints are required).\n",
    "* Code strategy: your `insertRecordsIntoTable()` just streams parquet → Postgres directly, which is optimal.\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Performance Optimization\n",
    "\n",
    "**Q7. What partitioning strategy should be used for the events table based on event\\_time to improve query performance?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* Use **range partitioning by month** on `event_time` (optional future step).\n",
    "* For now, with clean wide data, a **BRIN index** on `event_time` is likely enough.\n",
    "* Code strategy:\n",
    "\n",
    "  * Add BRIN index after table creation.\n",
    "  * If queries slow down, evolve to monthly partitions.\n",
    "\n",
    "---\n",
    "\n",
    "**Q8. How should we implement proper indexing for both transactional and analytical workloads?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* Transactional (row lookups): `BTREE` on PK (`event_id`).\n",
    "* Analytical: `BRIN` on `event_time`, `BTREE` on `user_id`, `product_id`, `event_type`.\n",
    "* Optional: composite indexes `(user_id, event_time)` and `(product_id, event_type)`.\n",
    "* Code strategy: add an `initIndexes()` method that runs after `createTable()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Q9. What materialized views or summary tables would accelerate common business intelligence queries?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* Since BI queries will hit 285M rows, create summary layers:\n",
    "\n",
    "  * `daily_revenue` (date, revenue, unique\\_customers).\n",
    "  * `product_perf` (product\\_id, views, carts, purchases, conversion\\_rate).\n",
    "  * `customer_rfm` (user\\_id, recency, frequency, monetary).\n",
    "* Refresh nightly with `REFRESH MATERIALIZED VIEW CONCURRENTLY`.\n",
    "* Code strategy: add SQL scripts to `answerQueryAnswer()` for generating these summaries.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427338f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Task 3 – Customer Engagement & Lifetime Analysis (Deep Dive: Q1–Q3)**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q1. Calculate Customer Lifetime Value (CLV) and Ranking within Segments**\n",
    "\n",
    "CLV represents the **total monetary value a customer generates** over their lifetime. Ranking within segments (e.g., `category_l1`) identifies the **highest-value customers per category**.\n",
    "\n",
    "**SQL & Analysis Strategy:**\n",
    "\n",
    "* **Step 1:** Aggregate purchases per customer and segment:\n",
    "\n",
    "  * Compute `SUM(price)` grouped by `user_id` and optionally `category_l1`.\n",
    "  * Ensures one row per customer (or customer-category pair).\n",
    "\n",
    "* **Step 2:** Rank customers within segments:\n",
    "\n",
    "  * Use `RANK()` or `DENSE_RANK()` window function.\n",
    "  * Partition by `category_l1` and order by CLV descending.\n",
    "\n",
    "* **Step 3 (Optional):** Identify top-tier customers:\n",
    "\n",
    "  * Filter by rank ≤ 10 or a percentile cutoff (e.g., top 20%).\n",
    "\n",
    "**Business Implication:**\n",
    "\n",
    "* High-ranking customers → target for **premium offers or loyalty programs**.\n",
    "* Low CLV customers → opportunities to **increase engagement** or reduce churn.\n",
    "\n",
    "**Key SQL Functions / Concepts:**\n",
    "\n",
    "* `SUM(price)` (aggregation)\n",
    "* `GROUP BY user_id [, category_l1]`\n",
    "* `RANK() OVER (PARTITION BY category_l1 ORDER BY SUM(price) DESC)` (window function)\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2. Identify Customers with Longest Gaps Between Purchase Events**\n",
    "\n",
    "Long gaps in purchase behavior indicate **low engagement or churn risk**. Measuring gaps allows **retention-focused interventions**.\n",
    "\n",
    "**SQL & Analysis Strategy:**\n",
    "\n",
    "* **Step 1:** Filter only purchase events:\n",
    "\n",
    "  * `event_type = 'purchase'` or `has_purchase = TRUE`.\n",
    "\n",
    "* **Step 2:** Compute consecutive purchase gaps:\n",
    "\n",
    "  * Use `LAG(event_time) OVER (PARTITION BY user_id ORDER BY event_time)` to get previous purchase.\n",
    "  * Calculate gap: `event_time - previous_event_time`.\n",
    "\n",
    "* **Step 3:** Aggregate per user:\n",
    "\n",
    "  * Compute `MAX(gap)` per `user_id` to find the **longest gap**.\n",
    "\n",
    "* **Step 4 (Optional):** Rank customers by longest gap:\n",
    "\n",
    "  * Identify users at **highest churn risk**.\n",
    "\n",
    "**Business Implication:**\n",
    "\n",
    "* Customers with **long gaps** → candidates for **re-engagement campaigns**.\n",
    "* Short gaps → highly engaged users; monitor for cross-sell opportunities.\n",
    "\n",
    "**Key SQL Functions / Concepts:**\n",
    "\n",
    "* `LAG()` (window function for previous value)\n",
    "* `MAX()` (aggregation)\n",
    "* `PARTITION BY user_id ORDER BY event_time`\n",
    "* Timestamp arithmetic (`event_time - previous_event_time`)\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3. Rolling 30-Day Active Users & Month-over-Month Retention**\n",
    "\n",
    "Rolling metrics capture **trends in user engagement**, while MoM retention evaluates **returning user behavior**.\n",
    "\n",
    "**SQL & Analysis Strategy:**\n",
    "\n",
    "**Step A – Rolling 30-Day Active Users**\n",
    "\n",
    "* **Step 1:** Aggregate daily active users:\n",
    "\n",
    "  * `COUNT(DISTINCT user_id)` grouped by `DATE(event_time)`.\n",
    "\n",
    "* **Step 2:** Compute rolling 30-day active users:\n",
    "\n",
    "  * Use **window function**:\n",
    "\n",
    "    * `SUM(daily_users) OVER (ORDER BY day ROWS BETWEEN 29 PRECEDING AND CURRENT ROW)`\n",
    "  * Ensures **current day + previous 29 days** are included.\n",
    "\n",
    "* **Step 3 (Optional):** Fill missing days if needed:\n",
    "\n",
    "  * `generate_series()` to ensure **continuous daily timeline**.\n",
    "\n",
    "**Step B – Month-over-Month (MoM) Retention**\n",
    "\n",
    "* **Step 1:** Bucket users by month: `DATE_TRUNC('month', event_time)`.\n",
    "* **Step 2:** Identify users active in current and previous month.\n",
    "* **Step 3:** Calculate retention rate:\n",
    "\n",
    "$$\n",
    "Retention_{MoM} = \\frac{COUNT(users_{current} \\cap users_{previous})}{COUNT(users_{previous})}\n",
    "$$\n",
    "\n",
    "**Business Implication:**\n",
    "\n",
    "* Rolling 30-day users → monitor **trends and spikes in engagement**.\n",
    "* MoM retention → evaluate **loyalty, retention programs**, and growth opportunities.\n",
    "\n",
    "**Key SQL Functions / Concepts:**\n",
    "\n",
    "* `COUNT(DISTINCT user_id)` (unique users)\n",
    "* `DATE(event_time)` / `DATE_TRUNC('month', event_time)` (time bucketing)\n",
    "* Window functions: `SUM(...) OVER (...)` for rolling metrics\n",
    "* Self-joins or CTEs for retention calculations\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Q1:\n",
    "# -- WITH customer_clv AS (\n",
    "# --     SELECT\n",
    "# --         user_id,\n",
    "# --         category_l1,\n",
    "# --         SUM(price) AS clv\n",
    "# --     FROM ecommerce_events\n",
    "# --     GROUP BY user_id, category_l1\n",
    "# -- )\n",
    "\n",
    "# -- SELECT\n",
    "# --     user_id,\n",
    "# --     category_l1,\n",
    "# --     clv,\n",
    "# --     RANK() OVER (PARTITION BY category_l1 ORDER BY clv DESC) AS rank_within_category\n",
    "# -- FROM customer_clv\n",
    "# -- ORDER BY category_l1, rank_within_category\n",
    "# -- LIMIT 500;\n",
    "\n",
    "# -- Q2:\n",
    "# -- WITH customer_lag AS (\n",
    "# -- \tselect\n",
    "# -- \tuser_id,\n",
    "# -- \tevent_time,\n",
    "# -- \tLAG(event_time) OVER (\n",
    "# -- \t  PARTITION BY user_id\n",
    "# -- \t  ORDER BY event_time) AS previous_event_time\n",
    "# -- \tfrom ecommerce_events\n",
    "# -- \twhere event_type = 'purchase'\n",
    "# -- \torder by event_time asc\n",
    "# -- )\n",
    "\n",
    "# -- select user_id, \n",
    "# -- max(event_time - previous_event_time) AS gap\n",
    "# -- from customer_lag where previous_event_time IS NOT NULL\n",
    "# -- group by user_id\n",
    "# -- order by gap desc\n",
    "\n",
    "# -- Q3:\n",
    "# -- WITH customer_rolling AS(\n",
    "# -- SELECT DATE(event_time) AS day,\n",
    "# -- COUNT(DISTINCT user_id) AS daily_users\n",
    "# -- from ecommerce_events\n",
    "# -- group by day\n",
    "# -- )\n",
    "\n",
    "# -- select\n",
    "# -- day,\n",
    "# -- SUM(daily_users) OVER (\n",
    "# --         ORDER BY day\n",
    "# --         ROWS BETWEEN 29 PRECEDING AND CURRENT ROW\n",
    "# --     ) AS rolling_30d_active_users\n",
    "# -- from customer_rolling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b3ae46",
   "metadata": {},
   "source": [
    "\n",
    "## **Task 2.2 – Product Performance & Cross-Selling (Q4–Q6)**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q4: Products with Highest Conversion Rate (View → Purchase)**\n",
    "\n",
    "**Objective:**\n",
    "Identify products that **most efficiently convert views into purchases**, i.e., which products have the highest probability that a view results in a purchase.\n",
    "\n",
    "**Analytical Strategy:**\n",
    "\n",
    "* **Step 1 – Count views and purchases per product:**\n",
    "\n",
    "  * Count events where `event_type = 'view'` → denominator.\n",
    "  * Count events where `event_type = 'purchase'` → numerator.\n",
    "\n",
    "* **Step 2 – Calculate conversion rate:**\n",
    "\n",
    "$$\n",
    "\\text{Conversion Rate} = \\frac{\\text{Number of Purchases}}{\\text{Number of Views}}\n",
    "$$\n",
    "\n",
    "* **Step 3 – Handle edge cases:**\n",
    "\n",
    "  * Avoid division by zero (products with zero views).\n",
    "\n",
    "* **Step 4 – Rank products by conversion rate:**\n",
    "\n",
    "  * Helps identify **highly effective products** for marketing and inventory decisions.\n",
    "\n",
    "**Business Implication:**\n",
    "\n",
    "* High-conversion products → good candidates for promotion, upselling, or bundling.\n",
    "* Low-conversion products → may require better product detail pages or targeted campaigns.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q5: Product Category Performance Hierarchies Using CTEs**\n",
    "\n",
    "**Objective:**\n",
    "Analyze **product performance across category hierarchies** (product → category_l3 → category_l2 → category_l1) for revenue and conversion insights.\n",
    "\n",
    "**Analytical Strategy:**\n",
    "\n",
    "* **Step 1 – Aggregate metrics per product:**\n",
    "\n",
    "  * Count views, count purchases, sum revenue, compute conversion rate.\n",
    "\n",
    "* **Step 2 – Aggregate metrics per category level:**\n",
    "\n",
    "  * Repeat the same aggregation for `category_l3`, `category_l2`, and `category_l1`.\n",
    "  * Use **CTEs** to organize calculations for each level.\n",
    "\n",
    "* **Step 3 – Optional:**\n",
    "\n",
    "  * Query any level as needed (product, category_l3, etc.) using the precomputed CTEs.\n",
    "\n",
    "**Business Implication:**\n",
    "\n",
    "* Understand **which categories and products drive revenue and conversion**.\n",
    "* Helps with **inventory planning, promotion targeting, and category-level KPIs**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q6: Product Affinity Scores & Cross-Selling Opportunities**\n",
    "\n",
    "**Objective:**\n",
    "Identify **products frequently purchased together** to inform cross-selling, bundling, and recommendation strategies.\n",
    "\n",
    "**Analytical Strategy:**\n",
    "\n",
    "* **Step 1 – Identify multi-product purchase sessions:**\n",
    "\n",
    "  * Aggregate `product_id`s per `user_session`.\n",
    "  * Filter sessions with more than 1 purchase.\n",
    "\n",
    "* **Step 2 – Unnest products into rows:**\n",
    "\n",
    "  * Convert arrays of products into separate rows per session.\n",
    "\n",
    "* **Step 3 – Generate product pairs per session:**\n",
    "\n",
    "  * Self-join on `user_session` to get all product combinations.\n",
    "  * Keep unique pairs (`product1 < product2`) to avoid duplicates.\n",
    "\n",
    "* **Step 4 – Aggregate pair counts across sessions:**\n",
    "\n",
    "  * Count sessions containing both products → numerator.\n",
    "  * Count sessions containing `product1` → denominator.\n",
    "\n",
    "* **Step 5 – Compute affinity score:**\n",
    "\n",
    "$$\n",
    "\\text{Affinity Score} = \\frac{\\text{Sessions with both products}}{\\text{Sessions with product1}}\n",
    "$$\n",
    "\n",
    "* **Step 6 – Interpret results:**\n",
    "\n",
    "  * Score ≈ 1 → products always bought together → strong cross-sell.\n",
    "  * Score ≈ 0 → products rarely bought together → weak cross-sell.\n",
    "\n",
    "**Business Implication:**\n",
    "\n",
    "* High-affinity pairs → promote as bundles or recommendations.\n",
    "* Low-affinity pairs → explore new cross-sell opportunities or ignore.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Q1\n",
    "# -- SELECT product_id,\n",
    "# -- round((COUNT(event_type) FILTER (WHERE event_type = 'purchase')* 1.0/\n",
    "# -- NULLIF(COUNT(event_type) FILTER (WHERE event_type = 'view'), 0)), 5) as conversion_rate\n",
    "# -- FROM ecommerce_events\n",
    "# -- group by product_id\n",
    "\n",
    "\n",
    "\n",
    "# -- Q2\n",
    "# -- WITH product_level_stats AS(\n",
    "# -- \tSELECT\n",
    "# -- \tproduct_id,\n",
    "# -- \tCOUNT(event_type) FILTER (WHERE event_type = 'view') as views_count,\n",
    "# -- \tCOUNT(event_type) FILTER (WHERE event_type = 'purchase') as purchases_count,\n",
    "# -- \tSUM(price) as reveneue,\n",
    "# -- \tround((COUNT(event_type) FILTER (WHERE event_type = 'purchase')* 1.0/\n",
    "# -- \tNULLIF(COUNT(event_type) FILTER (WHERE event_type = 'view'), 0)), 5) as conversion_rate\n",
    "# -- \tFROM ecommerce_events\n",
    "# -- \tgroup by product_id\n",
    "# -- \t),\n",
    "# -- category_l3_stats AS(\n",
    "# -- \tSELECT\n",
    "# -- \tcategory_l3,\n",
    "# -- \tCOUNT(event_type) FILTER (WHERE event_type = 'view') as views_count,\n",
    "# -- \tCOUNT(event_type) FILTER (WHERE event_type = 'purchase') as purchases_count,\n",
    "# -- \tSUM(price) as reveneue,\n",
    "# -- \tround((COUNT(event_type) FILTER (WHERE event_type = 'purchase')* 1.0/\n",
    "# -- \tNULLIF(COUNT(event_type) FILTER (WHERE event_type = 'view'), 0)), 5) as conversion_rate\n",
    "# -- \tFROM ecommerce_events\n",
    "# -- \tgroup by category_l3\n",
    "# -- \t),\n",
    "# -- category_l2_stats AS(\n",
    "# -- \tSELECT\n",
    "# -- \tcategory_l2,\n",
    "# -- \tCOUNT(event_type) FILTER (WHERE event_type = 'view') as views_count,\n",
    "# -- \tCOUNT(event_type) FILTER (WHERE event_type = 'purchase') as purchases_count,\n",
    "# -- \tSUM(price) as reveneue,\n",
    "# -- \tround((COUNT(event_type) FILTER (WHERE event_type = 'purchase')* 1.0/\n",
    "# -- \tNULLIF(COUNT(event_type) FILTER (WHERE event_type = 'view'), 0)), 5) as conversion_rate\n",
    "# -- \tFROM ecommerce_events\n",
    "# -- \tgroup by category_l2\n",
    "# -- \t),\n",
    "# -- category_l1_stats AS(\n",
    "# -- \tSELECT\n",
    "# -- \tcategory_l1,\n",
    "# -- \tCOUNT(event_type) FILTER (WHERE event_type = 'view') as views_count,\n",
    "# -- \tCOUNT(event_type) FILTER (WHERE event_type = 'purchase') as purchases_count,\n",
    "# -- \tSUM(price) as reveneue,\n",
    "# -- \tround((COUNT(event_type) FILTER (WHERE event_type = 'purchase')* 1.0/\n",
    "# -- \tNULLIF(COUNT(event_type) FILTER (WHERE event_type = 'view'), 0)), 5) as conversion_rate\n",
    "# -- \tFROM ecommerce_events\n",
    "# -- \tgroup by category_l1\n",
    "# -- \t)\n",
    "\n",
    "\n",
    "\n",
    "# -- Q3\n",
    "# -- with purchase_sessions as (\n",
    "# -- \tselect user_session,\n",
    "# -- \tARRAY_AGG(product_id) as products_arr,\n",
    "# -- \tCOUNT(event_type) FILTER (WHERE event_type = 'purchase') as purchases_count\n",
    "# -- \tfrom ecommerce_events\n",
    "# -- \tgroup by user_session\n",
    "# -- ),\n",
    "\n",
    "# -- purchase_sessions_multi as (\n",
    "# -- select user_session, UNNEST(products_arr) as product_id from purchase_sessions\n",
    "# -- where purchases_count > 1\n",
    "# -- ),\n",
    "\n",
    "# -- purchase_cross as (SELECT \n",
    "# --         p1.user_session, \n",
    "# --         p1.product_id AS product1, \n",
    "# --         p2.product_id AS product2\n",
    "# -- from purchase_sessions_multi p1 join purchase_sessions_multi p2\n",
    "# -- on p1.user_session = p2.user_session where p1.product_id < p2.product_id\n",
    "# -- ),\n",
    "\n",
    "# -- pair_counts AS (\n",
    "# --     SELECT \n",
    "# --         product1, \n",
    "# --         product2, \n",
    "# --         COUNT(DISTINCT user_session) AS pair_sessions\n",
    "# --     FROM purchase_cross\n",
    "# --     GROUP BY product1, product2\n",
    "# -- ),\n",
    "\n",
    "# -- product_sessions AS (\n",
    "# --     SELECT \n",
    "# --         product_id, \n",
    "# --         COUNT(DISTINCT user_session) AS product_sessions\n",
    "# --     FROM purchase_sessions_multi\n",
    "# --     GROUP BY product_id\n",
    "# -- )\n",
    "# -- SELECT \n",
    "# --     pc.product1,\n",
    "# --     pc.product2,\n",
    "# --     pc.pair_sessions,\n",
    "# --     ps.product_sessions,\n",
    "# --     ROUND(pc.pair_sessions::numeric / ps.product_sessions, 5) AS affinity_score\n",
    "# -- FROM pair_counts pc\n",
    "# -- JOIN product_sessions ps\n",
    "# --   ON pc.product1 = ps.product_id\n",
    "# -- ORDER BY affinity_score DESC\n",
    "# -- LIMIT 50;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90204099",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Q7: Identify Seasonal Trends & YoY Growth** *(not applicable for 1-month data, but structured approach if multi-year data were available)*\n",
    "\n",
    "**Objective:**\n",
    "Identify seasonal patterns in purchases or views and compute **year-over-year growth**.\n",
    "\n",
    "**Analytical Strategy:**\n",
    "\n",
    "* **Step 1 – Aggregate data by period:**\n",
    "\n",
    "  * Aggregate `event_type = 'purchase'` by `MONTH(event_time)` and `YEAR(event_time)` (or by week/day for intra-month trends).\n",
    "  * Compute counts or revenue per period.\n",
    "\n",
    "* **Step 2 – Compute growth rates:**\n",
    "  $$\n",
    "  \\text{YoY Growth} = \\frac{\\text{Current Period Value} - \\text{Value in Same Period Last Year}}{\\text{Value in Same Period Last Year}}\n",
    "  $$\n",
    "\n",
    "  * For seasonal trends, plot counts by month/week to see spikes.\n",
    "\n",
    "* **Step 3 – Handle edge cases:**\n",
    "\n",
    "  * Missing data for some months → treat as 0 or NULL.\n",
    "  * Consider outlier days (e.g., Black Friday) separately to avoid skewing averages.\n",
    "\n",
    "**Business Interpretation:**\n",
    "\n",
    "* Helps marketing plan **campaigns aligned with seasonal spikes**.\n",
    "* Inventory planning: stock more during high-season months.\n",
    "* Detect long-term growth or decline trends.\n",
    "\n",
    "**SQL Structure Guidance:**\n",
    "\n",
    "* Use a **CTE** to aggregate counts/revenue per period.\n",
    "* Use `LAG()` or joins to calculate YoY growth.\n",
    "* Optional: `ROW_NUMBER()` or `RANK()` to detect top seasonal months.\n",
    "\n",
    "---\n",
    "\n",
    "## **Q8: Peak Shopping Hours & Day-of-Week Patterns**\n",
    "\n",
    "**Objective:**\n",
    "Understand **when users engage most** (hour of day or weekday), both in terms of volume and efficiency.\n",
    "\n",
    "**Analytical Strategy:**\n",
    "\n",
    "* **Step 1 – Aggregate events by hour/day:**\n",
    "\n",
    "  * Use `sessionStartHour` and `sessionDayOfWeek`.\n",
    "  * Count `views` and `purchases`.\n",
    "\n",
    "* **Step 2 – Calculate distribution and conversion rate:**\n",
    "  $$\n",
    "  \\text{Conversion Rate} = \\frac{\\text{Purchases in Hour/Day}}{\\text{Views in Hour/Day}}\n",
    "  $$\n",
    "\n",
    "  * Compute share of total purchases and views per hour/day.\n",
    "\n",
    "* **Step 3 – Handle edge cases:**\n",
    "\n",
    "  * Missing hours/days → fill zeros or include all hours 0–23 and weekdays Mon–Sun.\n",
    "  * Adjust for partial sessions at midnight.\n",
    "\n",
    "**Business Interpretation:**\n",
    "\n",
    "* Timing of **push notifications or campaigns**.\n",
    "* Allocate support or logistics resources during peak hours.\n",
    "* Detect unusual spikes in off-peak hours (possible fraud or bots).\n",
    "\n",
    "**SQL Structure Guidance:**\n",
    "\n",
    "* **CTEs** to aggregate purchases and views separately.\n",
    "* **Window functions** to calculate shares and ranks.\n",
    "* **Join** views and purchases to compute conversion rate per hour/day.\n",
    "\n",
    "---\n",
    "\n",
    "## **Q9: Moving Averages & Anomaly Detection**\n",
    "\n",
    "**Objective:**\n",
    "Smooth daily purchase/revenue trends and **identify anomalies** (unexpected spikes or drops).\n",
    "\n",
    "**Analytical Strategy:**\n",
    "\n",
    "* **Step 1 – Aggregate daily metrics:**\n",
    "\n",
    "  * Count `session_count` or sum `daily_revenue` grouped by `DATE(event_time)`.\n",
    "\n",
    "* **Step 2 – Compute moving average:**\n",
    "  $$\n",
    "  \\text{MA}*n = \\frac{\\text{Day}*{t-n+1} + \\ldots + \\text{Day}_t}{n}\n",
    "  $$\n",
    "\n",
    "  * Use a 3–7 day rolling window depending on dataset length.\n",
    "\n",
    "* **Step 3 – Identify anomalies:**\n",
    "  $$\n",
    "  \\text{Deviation} = \\frac{\\text{Actual} - \\text{MA}}{\\text{MA}}\n",
    "  $$\n",
    "\n",
    "  * Flag days where |Deviation| > threshold (e.g., 30%).\n",
    "  * Optionally detect both spikes and drops.\n",
    "\n",
    "* **Step 4 – Handle edge cases:**\n",
    "\n",
    "  * First `n-1` days → partial window.\n",
    "  * Low volume days → may appear as anomalies naturally.\n",
    "  * Consider separating out special promo/holiday days.\n",
    "\n",
    "**Business Interpretation:**\n",
    "\n",
    "* Alerts marketing, operations, and finance teams to unusual trends.\n",
    "* Supports **forecasting and capacity planning**.\n",
    "* Helps detect fraud or technical issues if spikes/drops are unexplained.\n",
    "\n",
    "**SQL Structure Guidance:**\n",
    "\n",
    "* **CTE 1:** Aggregate daily counts and revenue.\n",
    "* **CTE 2:** Compute moving average using `AVG() OVER (ORDER BY event_date ROWS BETWEEN n-1 PRECEDING AND CURRENT ROW)`.\n",
    "* **Final SELECT:** Compute deviation and flag anomalies using `CASE WHEN ABS(...) > threshold THEN TRUE ELSE FALSE END`.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd4029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1\n",
    "# unapplicable\n",
    "\n",
    "# Q2\n",
    "# WITH sessions_per_hour_purchase AS (\n",
    "# \tSELECT\n",
    "# \t\tsessionStartHour,\n",
    "# \t\tCOUNT(user_session) as session_count\n",
    "# \tFROM\n",
    "# \t\tecommerce_events\n",
    "# \tWHERE\n",
    "# \t\tevent_type = 'purchase'\n",
    "# \tGROUP BY\n",
    "# \t\tsessionStartHour\n",
    "# ),\n",
    "\n",
    "# sessions_per_hour_view AS (\n",
    "# \tSELECT\n",
    "# \t\tsessionStartHour,\n",
    "# \t\tCOUNT(user_session) as session_count\n",
    "# \tFROM\n",
    "# \t\tecommerce_events\n",
    "# \tWHERE\n",
    "# \t\tevent_type = 'view'\n",
    "# \tGROUP BY\n",
    "# \t\tsessionStartHour\n",
    "# )\n",
    "\n",
    "# SELECT\n",
    "# \tp.sessionStartHour,\n",
    "# \tp.session_count * 1.0 / SUM(p.session_count)\n",
    "# \tover () AS purchase_share,\n",
    "# \tv.session_count * 1.0 / SUM(v.session_count)\n",
    "# \tover () AS view_share,\n",
    "# \tp.session_count * 1.0 / v.session_count as conversion_rate_per_day\n",
    "# FROM\n",
    "# \tsessions_per_hour_purchase p\n",
    "# FULL JOIN\n",
    "# \tsessions_per_hour_view v\n",
    "# ON\n",
    "# \tp.sessionStartHour = v.sessionStartHour\n",
    "# ORDER BY\n",
    "# \tsessionStartHour asc\n",
    "\n",
    "\n",
    "# WITH sessions_per_day_purchase AS (\n",
    "# \tSELECT\n",
    "# \t\tsessionDayOfWeek,\n",
    "# \t\tCOUNT(user_session) as session_count\n",
    "# \tFROM\n",
    "# \t\tecommerce_events\n",
    "# \tWHERE\n",
    "# \t\tevent_type = 'purchase'\n",
    "# \tGROUP BY\n",
    "# \t\tsessionDayOfWeek\n",
    "# ),\n",
    "\n",
    "# sessions_per_day_views AS (\n",
    "# \tSELECT\n",
    "# \t\tsessionDayOfWeek,\n",
    "# \t\tCOUNT(user_session) as session_count\n",
    "# \tFROM\n",
    "# \t\tecommerce_events\n",
    "# \tWHERE\n",
    "# \t\tevent_type = 'view'\n",
    "# \tGROUP BY\n",
    "# \t\tsessionDayOfWeek\n",
    "# )\n",
    "\n",
    "# SELECT\n",
    "# \tp.sessionDayOfWeek,\n",
    "# \tp.session_count * 1.0 / SUM(p.session_count)\n",
    "# \tover () AS purchase_share,\n",
    "# \tv.session_count * 1.0 / SUM(v.session_count)\n",
    "# \tover () AS view_share,\n",
    "# \tp.session_count * 1.0 / v.session_count as conversion_rate_per_day\n",
    "# FROM\n",
    "# \tsessions_per_day_purchase p\n",
    "# FULL JOIN \n",
    "# \tsessions_per_day_views v\n",
    "# ON\n",
    "# \tp.sessionDayOfWeek = v.sessionDayOfWeek\n",
    "# ORDER BY\n",
    "# \tconversion_rate_per_day desc\n",
    "\n",
    "# Q3\n",
    "# WITH daily_count AS (\n",
    "#     SELECT\n",
    "#         DATE(event_time) AS event_date,\n",
    "#         COUNT(user_session) AS session_count,\n",
    "#         SUM(price) AS daily_revenue\n",
    "#     FROM\n",
    "#         ecommerce_events\n",
    "#     WHERE\n",
    "#         event_type = 'purchase'\n",
    "#     GROUP BY event_date\n",
    "# ),\n",
    "# moving_average AS (\n",
    "#     SELECT\n",
    "#         event_date,\n",
    "#         session_count,\n",
    "#         daily_revenue,\n",
    "#         AVG(session_count) OVER (ORDER BY event_date\n",
    "#             ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS moving_avg_session,\n",
    "#         AVG(daily_revenue) OVER (ORDER BY event_date\n",
    "#             ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS moving_avg_revenue\n",
    "#     FROM\n",
    "#         daily_count\n",
    "# )\n",
    "# SELECT *,\n",
    "#     CASE \n",
    "#         WHEN ABS((session_count - moving_avg_session) / moving_avg_session) > 0.30\n",
    "#         THEN TRUE ELSE FALSE \n",
    "#     END AS is_anomaly_session,\n",
    "#     CASE \n",
    "#         WHEN ABS((daily_revenue - moving_avg_revenue) / moving_avg_revenue) > 0.30\n",
    "#         THEN TRUE ELSE FALSE \n",
    "#     END AS is_anomaly_revenue\n",
    "# FROM \n",
    "#     moving_average;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0af02c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Q1 – Implementing RFM Analysis**\n",
    "\n",
    "**Objective:**\n",
    "Compute **Recency, Frequency, and Monetary** metrics per customer to quantify engagement and value.\n",
    "\n",
    "**Analytical Strategy:**\n",
    "\n",
    "1. **Recency:**\n",
    "\n",
    "   * Days since last purchase: `CURRENT_DATE - MAX(event_time)` per customer.\n",
    "2. **Frequency:**\n",
    "\n",
    "   * Count of purchases per customer.\n",
    "3. **Monetary:**\n",
    "\n",
    "   * Total spend per customer.\n",
    "4. **Use of SQL window functions:**\n",
    "\n",
    "   * Can calculate percentiles (later used for segmentation).\n",
    "   * Aggregate metrics per customer using `GROUP BY`.\n",
    "\n",
    "**Solution Measures:**\n",
    "\n",
    "* Created a **materialized view** `customer_rfm_summary` for efficiency on 65M records.\n",
    "* Used **`FILTER` clauses** to count only purchases.\n",
    "* Handled edge cases like customers with zero purchases or null monetary values.\n",
    "\n",
    "**Business Interpretation:**\n",
    "\n",
    "* Provides a **baseline for customer value**.\n",
    "* Key input for segmentation, marketing campaigns, and retention strategies.\n",
    "\n",
    "---\n",
    "\n",
    "## **Q2 – Dynamic Customer Segmentation Based on Percentiles**\n",
    "\n",
    "**Objective:**\n",
    "Classify customers into segments (Power Users, Quick Buyers, Browsers/Bots, Disengaged) based on **purchase behavior**.\n",
    "\n",
    "**Analytical Strategy:**\n",
    "\n",
    "1. Compute **percentiles** (or numeric thresholds) for RFM metrics.\n",
    "2. **Define segments using `CASE WHEN`:**\n",
    "\n",
    "   * Power Users → recent, frequent, high spend\n",
    "   * Quick Buyers → recent, moderate frequency, moderate spend\n",
    "   * Browsers/Bots → low frequency & low spend\n",
    "   * Disengaged → remaining customers\n",
    "3. **Iterative adjustment:**\n",
    "\n",
    "   * Initially used percentile thresholds → didn’t work due to skewed data (many zeros).\n",
    "   * Switched to **numeric thresholds** derived from actual RFM distributions.\n",
    "4. **Order of `CASE WHEN` matters:**\n",
    "\n",
    "   * Ensure highest-value segments are matched first.\n",
    "\n",
    "**Solution Measures:**\n",
    "\n",
    "* Used **numeric thresholds**: e.g., `frequency > 5 AND monetary >= 500` for Power Users.\n",
    "* Validated segment distribution to ensure **all four segments exist**.\n",
    "\n",
    "**Business Interpretation:**\n",
    "\n",
    "* Identifies **high-value vs low-value customers**.\n",
    "* Useful for marketing, loyalty programs, promotions, and targeted campaigns.\n",
    "\n",
    "---\n",
    "\n",
    "## **Q3 – Customer Churn Probability Using Cohort Analysis**\n",
    "\n",
    "**Objective:**\n",
    "Estimate the probability that a customer stops purchasing over time using cohort analysis.\n",
    "\n",
    "**Analytical Strategy:**\n",
    "\n",
    "1. Define **cohorts** based on first purchase date.\n",
    "2. Track **retention over subsequent periods**.\n",
    "3. Calculate churn probability:\n",
    "\n",
    "[\n",
    "\\text{Churn Probability} = 1 - \\text{Retention Rate}\n",
    "]\n",
    "\n",
    "**Measures Taken:**\n",
    "\n",
    "* With only **one month of data**, true churn cannot be calculated.\n",
    "* Marked this analysis as **unapplicable**.\n",
    "* Optional alternative: compute **short-term engagement proxies** (e.g., single vs multiple purchases in the month).\n",
    "\n",
    "**Business Interpretation:**\n",
    "\n",
    "* True churn probability requires multi-period data.\n",
    "* Stakeholders would use it for retention campaigns and revenue forecasting once enough historical data is available.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3b0fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2\n",
    "# WITH customer_rfm_with_percentiles as (\n",
    "# SELECT *,\n",
    "#        PERCENT_RANK() OVER (ORDER BY recency ASC) AS recency_percentile,\n",
    "#        PERCENT_RANK() OVER (ORDER BY frequency DESC) AS frequency_percentile,\n",
    "#        PERCENT_RANK() OVER (ORDER BY monetary DESC) AS monetary_percentile\n",
    "# FROM customer_rfm\n",
    "# ),\n",
    "# customer_segments AS (\n",
    "# SELECT *,\n",
    "#     CASE\n",
    "#         WHEN frequency > 5\n",
    "#              AND monetary >= 500 THEN 'Power Users'\n",
    "#         WHEN frequency BETWEEN 1 AND 5\n",
    "#              AND monetary >= 100 THEN 'Quick Buyers'\n",
    "#         WHEN frequency = 0\n",
    "#              OR monetary < 100 THEN 'Browsers/Bots'\n",
    "#         ELSE 'Disengaged Users'\n",
    "#     END AS customer_segment\n",
    "# FROM customer_rfm_summary\n",
    "# )\n",
    "# SELECT customer_segment,\n",
    "#        COUNT(*) AS num_users\n",
    "# FROM customer_segments\n",
    "# GROUP BY customer_segment\n",
    "# ORDER BY num_users DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53cee09",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Task 3 – Advanced Aggregations & Reporting (Q7–Q9)**\n",
    "\n",
    "---\n",
    "\n",
    "### **Q7: Using PIVOT Operations for Category Performance by Month**\n",
    "\n",
    "**Objective:**\n",
    "Transform detailed event-level data into a **cross-tab format**, where each row is a category and each column represents a month, showing a metric like revenue or purchase count.\n",
    "\n",
    "**Analytical Strategy:**\n",
    "\n",
    "1. **Step 1 – Identify the grouping and measure:**\n",
    "\n",
    "   * Group by `category_l1` (or `category_l2/l3`).\n",
    "   * Metric: total purchases, revenue, or session count.\n",
    "2. **Step 2 – Check time granularity:**\n",
    "\n",
    "   * Extract month from `event_time`.\n",
    "   * Must have **multiple months** for pivot to be meaningful.\n",
    "3. **Step 3 – Use PIVOT / conditional aggregation:**\n",
    "\n",
    "   * If the SQL dialect supports `PIVOT`, use it.\n",
    "   * Otherwise, use `CASE WHEN MONTH(event_time) = X THEN metric END` with `SUM()`.\n",
    "4. **Step 4 – Edge cases:**\n",
    "\n",
    "   * Categories with zero activity in a month → fill with 0 or NULL.\n",
    "\n",
    "**Business Interpretation:**\n",
    "\n",
    "* Quickly see which categories perform best per month.\n",
    "* Identify seasonal trends and allocate inventory or marketing resources.\n",
    "\n",
    "**Applicability in our project:**\n",
    "\n",
    "* **Currently inapplicable** due to having only **one month of data**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q8: Funnel Conversion Rates (View → Cart → Purchase)**\n",
    "\n",
    "**Objective:**\n",
    "Calculate the **drop-off between funnel stages** for different segments (e.g., category, RFM group, traffic source).\n",
    "\n",
    "**Analytical Strategy:**\n",
    "\n",
    "1. **Step 1 – Aggregate event counts per stage:**\n",
    "\n",
    "   * Count `view`, `cart`, and `purchase` events per segment (`category_l1`, `brand`, or customer segment).\n",
    "2. **Step 2 – Calculate conversion rates:**\n",
    "\n",
    "   * View → Cart: `cart_count / view_count`\n",
    "   * Cart → Purchase: `purchase_count / cart_count`\n",
    "   * Full funnel: `purchase_count / view_count`\n",
    "   * Use `NULLIF()` to avoid division by zero.\n",
    "3. **Step 3 – Optional segmentation:**\n",
    "\n",
    "   * By category (`category_l1`) → shows product-level impact.\n",
    "   * By customer type (RFM segments) → shows behavioral differences.\n",
    "4. **Step 4 – Business interpretation:**\n",
    "\n",
    "   * Identify bottlenecks in the funnel.\n",
    "   * Prioritize interventions for high-impact segments.\n",
    "\n",
    "**SQL Implementation:**\n",
    "\n",
    "* We created a CTE `category_seg` that counts events per stage.\n",
    "* Calculated `vc_conversion_rate`, `cp_conversion_rate`, and `vp_conversion_rate`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q9: Statistical Functions for Product Price Analysis**\n",
    "\n",
    "**Objective:**\n",
    "Analyze **price distributions** and **relationships** with engagement metrics to inform pricing and merchandising decisions.\n",
    "\n",
    "**Analytical Strategy:**\n",
    "\n",
    "1. **Step 1 – Remove extreme/luxury outliers:**\n",
    "\n",
    "   * Use `isExtremeOutlier = false` and `isLuxuryItem = false`.\n",
    "2. **Step 2 – Grouping:**\n",
    "\n",
    "   * By `category_l1` or `(category_l1, brand)` to capture meaningful subgroups.\n",
    "3. **Step 3 – Apply statistical functions:**\n",
    "\n",
    "   * Average price: `AVG(price)`\n",
    "   * Price variability: `STDDEV(price)`\n",
    "   * Correlation with engagement metrics:\n",
    "\n",
    "     * `CORR(price, popularityScore)` → effect of price on product popularity\n",
    "     * Avoid `CORR(price, categoryConversionRate)` if variance is too low\n",
    "   * Use `FILTER (WHERE metric IS NOT NULL)` to prevent null issues.\n",
    "4. **Step 4 – Business interpretation:**\n",
    "\n",
    "   * Categories with high stddev → mixed pricing, may need segmentation.\n",
    "   * Strong correlation → pricing affects conversion/popularity.\n",
    "   * Weak correlation → other factors drive engagement; pricing less sensitive.\n",
    "\n",
    "**SQL Implementation:**\n",
    "\n",
    "* Aggregated by category and brand, filtered out outliers.\n",
    "* Applied `AVG`, `STDDEV`, and `CORR` with proper null handling.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7153a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1\n",
    "# inaplicable\n",
    "\n",
    "# Q2\n",
    "# WITH category_seg AS (\n",
    "#     SELECT\n",
    "#         category_l1,\n",
    "#         SUM(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) AS purchase_count,\n",
    "#         SUM(CASE WHEN event_type = 'view' THEN 1 ELSE 0 END) AS view_count,\n",
    "#         SUM(CASE WHEN event_type = 'cart' THEN 1 ELSE 0 END) AS cart_count\n",
    "#     FROM \n",
    "#         ecommerce_events\n",
    "#     GROUP BY\n",
    "#         category_l1\n",
    "# )\n",
    "# SELECT\n",
    "#     category_l1,\n",
    "#     cart_count * 1.0 / NULLIF(view_count, 0)   AS vc_conversion_rate, \n",
    "#     purchase_count * 1.0 / NULLIF(cart_count, 0) AS cp_conversion_rate, \n",
    "#     purchase_count * 1.0 / NULLIF(view_count, 0) AS vp_conversion_rate \n",
    "# FROM\n",
    "#     category_seg\n",
    "# ORDER BY\n",
    "#     vp_conversion_rate DESC;\n",
    "\n",
    "# Q3\n",
    "# SELECT\n",
    "#     category_l1,\n",
    "#     brand,\n",
    "#     AVG(price) AS price_average,\n",
    "#     STDDEV(price) AS price_std,\n",
    "#     CORR(price, popularityScore) \n",
    "#         FILTER (WHERE popularityScore IS NOT NULL) AS corr_price_vs_popularity_score\n",
    "# FROM\n",
    "#     ecommerce_events\n",
    "# WHERE\n",
    "#     isExtremeOutlier = false \n",
    "#     AND isLuxuryItem = false\n",
    "# GROUP BY \n",
    "#     category_l1, brand\n",
    "# ORDER BY\n",
    "#     category_l1, brand;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d6c1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
