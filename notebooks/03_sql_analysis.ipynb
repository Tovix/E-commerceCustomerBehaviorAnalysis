{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91071bc3",
   "metadata": {},
   "source": [
    "### **2.2 SQL Analysis**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Objectives**\n",
    "\n",
    "Demonstrate advanced SQL skills by creating a database-driven analysis workflow and answering complex business questions through structured queries.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7df247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "from psycopg2.extras import execute_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5443444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSqlIntegration(ABC):\n",
    "      \"\"\"\n",
    "      Abstract base class for bank marketing data SQL integration operations.\n",
    "\n",
    "      This class defines the interface for loading bank marketing data into a SQL database\n",
    "      and querying results. Concrete implementations must provide the actual database\n",
    "      interaction logic.\n",
    "\n",
    "      Attributes:\n",
    "            dataPath (str): Path to the data file to be processed\n",
    "      \"\"\"\n",
    "\n",
    "      def __init__(self, dataPath: str) -> None:\n",
    "            \"\"\"\n",
    "            Initialize the data integration handler.\n",
    "            \n",
    "            Args:\n",
    "            dataPath: Path to the bank marketing data file\n",
    "            \"\"\"\n",
    "            self.dataPath = dataPath\n",
    "\n",
    "      @abstractmethod\n",
    "      def createTable(self, connection: psycopg2.connect) -> None:\n",
    "            \"\"\"\n",
    "            Create the bank_marketing table in the database.\n",
    "            \n",
    "            Args:\n",
    "            connection: Active PostgreSQL database connection\n",
    "            \n",
    "            Raises:\n",
    "            psycopg2.Error: If table creation fails\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "      @abstractmethod\n",
    "      def answerQueryAnswer(self, connection:psycopg2.connect, queryString: str, index: List[str]) -> pd.DataFrame:\n",
    "            \"\"\"\n",
    "            Convert SQL query results into a pandas DataFrame.\n",
    "            \n",
    "            Args:\n",
    "            queryRows: List of tuples from SQL query results\n",
    "            index: List of index values for the DataFrame\n",
    "            \n",
    "            Returns:\n",
    "            pd.DataFrame: Formatted DataFrame from query results\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "      @abstractmethod\n",
    "      def insertRecordsIntoTable(self, connection: psycopg2.connect, alreadyAdded: bool, batch_size: int = 100_000) -> None:\n",
    "            \"\"\"\n",
    "            Insert records from the data file into the database table.\n",
    "\n",
    "            Args:\n",
    "            connection: Active PostgreSQL database connection\n",
    "\n",
    "            Raises:\n",
    "            psycopg2.Error: If data insertion fails\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "\n",
    "class EcommerceDataSqlIntegration(DataSqlIntegration):\n",
    "      \"\"\"\n",
    "      Concrete implementation of bank marketing data SQL integration.\n",
    "      \n",
    "      Provides PostgreSQL-specific implementation for loading bank marketing campaign data\n",
    "      and converting query results to pandas DataFrames.\n",
    "      \"\"\"\n",
    "      \n",
    "      def createTable(self, connection: psycopg2.connect) -> None:\n",
    "            \"\"\"\n",
    "            Create the ecommerce_events table with proper schema if it doesn't exist.\n",
    "            \"\"\"\n",
    "            cursor = connection.cursor()\n",
    "            try:\n",
    "                  cursor.execute(\"\"\"\n",
    "                        CREATE TABLE IF NOT EXISTS ecommerce_events (\n",
    "                        event_id BIGSERIAL PRIMARY KEY,\n",
    "                        event_time TIMESTAMPTZ NOT NULL,\n",
    "                        event_type VARCHAR(20) NOT NULL CHECK (event_type IN ('view', 'cart', 'purchase')),\n",
    "                        product_id BIGINT,\n",
    "                        category_id BIGINT,\n",
    "                        brand TEXT,\n",
    "                        price NUMERIC(12,2),\n",
    "                        user_id BIGINT,\n",
    "                        user_session TEXT,\n",
    "\n",
    "                        isFreeItem BOOLEAN,\n",
    "                        isLuxuryItem BOOLEAN,\n",
    "                        isExtremeOutlier BOOLEAN,\n",
    "                        isAbnormal BOOLEAN,\n",
    "                        has_purchase BOOLEAN,\n",
    "                        isMultiDaySession BOOLEAN,\n",
    "                        hasCategoryCode BOOLEAN,\n",
    "\n",
    "                        category_l1 TEXT,\n",
    "                        category_l2 TEXT,\n",
    "                        category_l3 TEXT,\n",
    "                        isLowFrequencyCategory BOOLEAN,\n",
    "\n",
    "                        sessionDuration REAL,\n",
    "                        sessionStartHour SMALLINT,\n",
    "                        sessionDayOfWeek TEXT,\n",
    "                        isWeekendSession BOOLEAN,\n",
    "                        isMidnightActivity BOOLEAN,\n",
    "                        isDirectPurchase BOOLEAN,\n",
    "                        isDirectPurchase_right BOOLEAN,\n",
    "                        isAbandonedCart BOOLEAN,\n",
    "                        advancedToCart BOOLEAN,\n",
    "                        advancedToPurchase BOOLEAN,\n",
    "                        EngagementDepth INTEGER,\n",
    "\n",
    "                        popularityScore REAL,\n",
    "                        categoryConversionRate REAL,\n",
    "                        isInMultiCategories BOOLEAN\n",
    "                        );\n",
    "                  \"\"\")\n",
    "                  connection.commit()\n",
    "            except psycopg2.Error as e:\n",
    "                  connection.rollback()\n",
    "                  raise e\n",
    "\n",
    "      def addTableIndexes(self, connection: psycopg2.connect) -> None:\n",
    "            try:\n",
    "                  cursor = connection.cursor()\n",
    "                  cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_user_id ON ecommerce_events(user_id);\")\n",
    "                  cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_product_id ON ecommerce_events(product_id);\")\n",
    "                  cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_event_type ON ecommerce_events(event_type);\")\n",
    "                  cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_event_time_brin ON ecommerce_events USING BRIN(event_time);\")\n",
    "                  cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_user_event_time ON ecommerce_events(user_id, event_time);\")\n",
    "                  cursor.execute(\"ANALYZE ecommerce_events;\")\n",
    "                  connection.commit()\n",
    "\n",
    "            except psycopg2.Error as e:\n",
    "                  connection.rollback()\n",
    "            \n",
    "            finally:\n",
    "                  cursor.close()\n",
    "\n",
    "      def insertRecordsIntoTable(self, connection, alreadyAdded: bool, batchSize: int = 100_000):\n",
    "            if alreadyAdded:\n",
    "                  return\n",
    "\n",
    "            try:\n",
    "                  cursor = connection.cursor()\n",
    "\n",
    "                  # Lazy read Parquet\n",
    "                  lazy_df = pl.scan_parquet(self.dataPath)\n",
    "                  df = lazy_df.collect()\n",
    "                  total_rows = df.height\n",
    "\n",
    "                  # Iterate in chunks with progress bar\n",
    "                  for start in tqdm(range(0, total_rows, batchSize), desc=\"Inserting rows\"):\n",
    "                        end = min(start + batchSize, total_rows)\n",
    "                        chunk = df[start:end]\n",
    "                        data_tuples = [tuple(row) for row in chunk.to_numpy()]\n",
    "\n",
    "                        if data_tuples:\n",
    "                              execute_values(cursor, \"\"\"\n",
    "                                    INSERT INTO ecommerce_events (\n",
    "                                          event_time, event_type, product_id, category_id, brand, price, user_id, user_session,\n",
    "                                          isFreeItem, isLuxuryItem, isExtremeOutlier, isAbnormal, has_purchase, isMultiDaySession,\n",
    "                                          hasCategoryCode,\n",
    "                                          category_l1, category_l2, category_l3,\n",
    "                                          isLowFrequencyCategory,\n",
    "                                          sessionDuration, sessionStartHour, sessionDayOfWeek, isWeekendSession,\n",
    "                                          isMidnightActivity, isDirectPurchase, isDirectPurchase_right,\n",
    "                                          isAbandonedCart, advancedToCart, advancedToPurchase, EngagementDepth,\n",
    "                                          popularityScore, categoryConversionRate,\n",
    "                                          isInMultiCategories\n",
    "                                    ) VALUES %s\n",
    "                              \"\"\", data_tuples)\n",
    "                              connection.commit()\n",
    "\n",
    "            except Exception as e:\n",
    "                  connection.rollback()\n",
    "                  raise e\n",
    "            \n",
    "      def answerQueryAnswer(self, connection: psycopg2.connect, queryString: str, index: List[str]) -> pd.DataFrame:\n",
    "            \"\"\"\n",
    "            Execute SQL query and convert results to a properly formatted pandas DataFrame.\n",
    "            \n",
    "            Args:\n",
    "                  connection: Active PostgreSQL database connection\n",
    "            queryString: SQL query to execute\n",
    "            index: List of index values for the DataFrame\n",
    "            \n",
    "      Returns:\n",
    "            pd.DataFrame: Formatted DataFrame with query results\n",
    "            \n",
    "      Example:\n",
    "            >>> df = answerQueryAnswer(conn, \"SELECT age, balance FROM bank_marketing\", ['row1', 'row2'])\n",
    "      \"\"\"\n",
    "            cursor = connection.cursor()\n",
    "            try:\n",
    "                  cursor.execute(queryString)\n",
    "                  queryRows = cursor.fetchall()\n",
    "                  \n",
    "                  # Debug: Print raw query results\n",
    "                  print(f\"Query returned {len(queryRows)} rows\")\n",
    "                  if queryRows:\n",
    "                        print(\"First row sample:\", queryRows[0])\n",
    "                  \n",
    "                  # Create DataFrame with proper column handling\n",
    "                  if not queryRows:\n",
    "                        return pd.DataFrame(index=index)\n",
    "                        \n",
    "                  # Convert to DataFrame with column names\n",
    "                  df = pd.DataFrame.from_records(\n",
    "                        queryRows,\n",
    "                        columns=[desc[0] for desc in cursor.description],\n",
    "                        index=index[:len(queryRows)]  # Ensure index matches row count\n",
    "                  )\n",
    "                  return df\n",
    "                  \n",
    "            except psycopg2.Error as e:\n",
    "                  print(f\"Database error: {e}\")\n",
    "                  return pd.DataFrame()\n",
    "            finally:\n",
    "                  cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf196337",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSqlIntegration = EcommerceDataSqlIntegration(dataPath=\"D:\\programming\\Data Analysis\\E-commerceCustomerBehaviorAnalysis\\Data\\Processed\\prsc_fe9_nov_2019.parquet\")\n",
    "connection = psycopg2.connect(database=\"ecommerceCustomers\",host=\"localhost\",user=\"postgres\",\n",
    "                                    password=\"postgres\",port=\"5432\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c38386",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSqlIntegration.createTable(connection=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a0256",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSqlIntegration.insertRecordsIntoTable(connection=connection, alreadyAdded=False, batchSize=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e65e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSqlIntegration.addTableIndexes(connection=connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61a8021",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **Task 2.2.1 – Database Design and Data Loading**\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Database Schema Design\n",
    "\n",
    "**Q1. What is the optimal table structure to normalize the e-commerce data while maintaining query performance?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* Use a **single wide `ecommerce_events` table** (as you already did) because:\n",
    "\n",
    "  * Data is clean → no need for staging tables.\n",
    "  * Analysis and BI dashboards benefit from one denormalized table.\n",
    "* Keep dimensions (`products`, `customers`, `categories`) optional for future expansion, but not required now.\n",
    "* Code strategy: rely on your `createTable()` implementation that defines the schema with correct data types and constraints.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. How should we design indexes on user\\_id, product\\_id, and event\\_time to optimize common analytical queries?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* Create **BTREE indexes** on:\n",
    "\n",
    "  * `user_id` → for customer-level queries.\n",
    "  * `product_id` → for product performance analysis.\n",
    "  * `event_type` → for funnel/conversion queries.\n",
    "* Create **BRIN index** on `event_time` → for fast filtering by time ranges (cheap and scalable for 285M rows).\n",
    "* Optionally, add composite `(user_id, event_time)` for recency analysis.\n",
    "* Code strategy: extend `createTable()` to run `CREATE INDEX` after table creation.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. What foreign key relationships should be established between customers, products, and events tables?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* Since all analysis happens in the **single wide table**, foreign keys aren’t required for integrity (data is already clean).\n",
    "* If needed in the future:\n",
    "\n",
    "  * `events.user_id → customers.user_id`\n",
    "  * `events.product_id → products.product_id`\n",
    "* Code strategy: skip FK constraints for now to keep inserts fast; revisit if you add dimension tables later.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Data Import and Validation\n",
    "\n",
    "**Q4. How can we efficiently load 285 million records into a relational database with proper data type conversions?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* You already stream from parquet → Polars → numpy tuples → Postgres.\n",
    "* Optimize by:\n",
    "\n",
    "  * Using `execute_values()` in batches (you already do this).\n",
    "  * Tuning `batchSize` (100k is good; test 250k–500k for speed).\n",
    "  * Running with autocommit off + commit per batch (as you implemented).\n",
    "* Code strategy: keep your current `insertRecordsIntoTable()` logic.\n",
    "\n",
    "---\n",
    "\n",
    "**Q5. What SQL constraints should be implemented to ensure data quality during the import process?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* Since data is **already clean**, keep constraints minimal for performance:\n",
    "\n",
    "  * `NOT NULL` only where logically required (`event_time`, `event_type`).\n",
    "  * `CHECK` on `event_type IN ('view','cart','purchase')`.\n",
    "* Skip extra checks like duplicates or null validation (handled upstream in data pipeline).\n",
    "* Code strategy: your current `createTable()` is sufficient.\n",
    "\n",
    "---\n",
    "\n",
    "**Q6. How do we handle duplicate records and maintain referential integrity during bulk data loading?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* No duplicates exist → skip deduplication logic.\n",
    "* Referential integrity is not enforced (since no FK constraints are required).\n",
    "* Code strategy: your `insertRecordsIntoTable()` just streams parquet → Postgres directly, which is optimal.\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Performance Optimization\n",
    "\n",
    "**Q7. What partitioning strategy should be used for the events table based on event\\_time to improve query performance?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* Use **range partitioning by month** on `event_time` (optional future step).\n",
    "* For now, with clean wide data, a **BRIN index** on `event_time` is likely enough.\n",
    "* Code strategy:\n",
    "\n",
    "  * Add BRIN index after table creation.\n",
    "  * If queries slow down, evolve to monthly partitions.\n",
    "\n",
    "---\n",
    "\n",
    "**Q8. How should we implement proper indexing for both transactional and analytical workloads?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* Transactional (row lookups): `BTREE` on PK (`event_id`).\n",
    "* Analytical: `BRIN` on `event_time`, `BTREE` on `user_id`, `product_id`, `event_type`.\n",
    "* Optional: composite indexes `(user_id, event_time)` and `(product_id, event_type)`.\n",
    "* Code strategy: add an `initIndexes()` method that runs after `createTable()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Q9. What materialized views or summary tables would accelerate common business intelligence queries?**\n",
    "**A (Strategy):**\n",
    "\n",
    "* Since BI queries will hit 285M rows, create summary layers:\n",
    "\n",
    "  * `daily_revenue` (date, revenue, unique\\_customers).\n",
    "  * `product_perf` (product\\_id, views, carts, purchases, conversion\\_rate).\n",
    "  * `customer_rfm` (user\\_id, recency, frequency, monetary).\n",
    "* Refresh nightly with `REFRESH MATERIALIZED VIEW CONCURRENTLY`.\n",
    "* Code strategy: add SQL scripts to `answerQueryAnswer()` for generating these summaries.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ff6280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
